{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a908d626",
   "metadata": {},
   "source": [
    "# Environment Test\n",
    "Verify all dependencies and API connections are working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56507e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.13 (main, Jul 23 2025, 18:09:53) [GCC 12.2.0]\n",
      "Google API Key configured: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Google API Key configured: {'GOOGLE_API_KEY' in os.environ}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32163f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All core dependencies imported successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import langchain\n",
    "    import langgraph\n",
    "    import streamlit\n",
    "    import matplotlib\n",
    "    import plotly\n",
    "    print(\"✅ All core dependencies imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a925e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Configuración detectada:\n",
      "   Modelo: gemini-pro\n",
      "   Temperatura: 0.1\n",
      "   Max tokens: 8192\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised NotFound: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Google Gemini API error: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "💡 Verifica que GOOGLE_API_KEY esté configurada correctamente\n"
     ]
    }
   ],
   "source": [
    "# ✅ CONFIGURACIÓN ACTUALIZADA: Usando gemini-pro para optimizar rate limits\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from app.core.config import get_settings\n",
    "\n",
    "try:\n",
    "    # Usar la configuración centralizada como debe ser\n",
    "    settings = get_settings()\n",
    "    gemini_config = settings.gemini_config\n",
    "    \n",
    "    print(f\"🔧 Configuración detectada:\")\n",
    "    print(f\"   Modelo: {gemini_config['model_name']}\")\n",
    "    print(f\"   Temperatura: {gemini_config['temperature']}\")\n",
    "    print(f\"   Max output tokens: {gemini_config['max_output_tokens']}\")\n",
    "    print()\n",
    "    \n",
    "    # Crear LLM con la configuración correcta para Google Gemini\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=gemini_config[\"model_name\"],  # gemini-pro (actualizado para mejor rate limiting)\n",
    "        temperature=gemini_config[\"temperature\"],\n",
    "        max_output_tokens=gemini_config[\"max_output_tokens\"],  # Parámetro correcto para Google Gemini\n",
    "        google_api_key=gemini_config[\"api_key\"]\n",
    "    )\n",
    "    \n",
    "    response = llm.invoke(\"Hello, test connection\")\n",
    "    print(\"✅ Google Gemini API connection successful\")\n",
    "    print(f\"Response: {response.content[:100]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Google Gemini API error: {e}\")\n",
    "    print(\"💡 Verifica que GOOGLE_API_KEY esté configurada correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4534b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🏁 ENVIRONMENT TEST SUMMARY\n",
      "==================================================\n",
      "🐍 Python Version: 3.11.13\n",
      "🐳 Docker Environment: ✅ YES\n",
      "🔑 Google API Key: ✅ CONFIGURED\n",
      "🗄️  Database URL: ✅ CONFIGURED\n",
      "\n",
      "💡 Si todo está ✅, tu entorno está listo para el ReAct Agent!\n"
     ]
    }
   ],
   "source": [
    "# Environment Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🏁 ENVIRONMENT TEST SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"🐍 Python Version: {sys.version.split()[0]}\")\n",
    "print(f\"🐳 Docker Environment: {'✅ YES' if os.getenv('DATABASE_URL') else '❌ NO'}\")\n",
    "print(f\"🔑 Google API Key: {'✅ CONFIGURED' if os.getenv('GOOGLE_API_KEY') else '❌ MISSING'}\")\n",
    "print(f\"🗄️  Database URL: {'✅ CONFIGURED' if os.getenv('DATABASE_URL') else '❌ MISSING'}\")\n",
    "print(\"\\n💡 Si todo está ✅, tu entorno está listo para el ReAct Agent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634fa328",
   "metadata": {},
   "source": [
    "## 🎯 Actualización del Modelo Gemini\n",
    "\n",
    "**✅ MIGRACIÓN COMPLETADA**: Cambiado de `gemini-1.5-pro` a `gemini-pro` por optimización de rate limits\n",
    "\n",
    "### 📊 Motivos del Cambio:\n",
    "- **Rate Limits Mejorados**: `gemini-pro` ofrece 60 requests/minuto vs límites más estrictos de `1.5-pro`\n",
    "- **Costos Optimizados**: Modelo más económico para casos de uso de desarrollo y testing\n",
    "- **Compatibilidad**: Totalmente compatible con el stack actual de LangChain\n",
    "\n",
    "### 🔧 Cambios Realizados:\n",
    "- ✅ Configuración principal (`.env`) actualizada\n",
    "- ✅ Configuración por defecto (`config.py`) modificada  \n",
    "- ✅ Tests unitarios actualizados\n",
    "- ✅ Archivos de ejemplo sincronizados\n",
    "- ✅ Documentación actualizada\n",
    "\n",
    "### 🚀 Próximos Pasos:\n",
    "1. **Reiniciar servicios**: `make restart` para aplicar cambios\n",
    "2. **Ejecutar tests**: `make test` para validar funcionalidad\n",
    "3. **Monitorear performance**: Verificar que el nuevo modelo cumple expectativas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cb9c23",
   "metadata": {},
   "source": [
    "## ⚙️ Corrección Técnica: Parámetros de Google Gemini\n",
    "\n",
    "**🔧 PARÁMETRO CORREGIDO**: Uso de `max_output_tokens` en lugar de `max_tokens`\n",
    "\n",
    "### 📋 Diferencias Importantes:\n",
    "- **OpenAI LangChain**: Usa `max_tokens` \n",
    "- **Google Gemini LangChain**: Usa `max_output_tokens`\n",
    "- **Motivo**: Cada proveedor tiene su propia implementación en LangChain\n",
    "\n",
    "### ✅ Cambios Aplicados:\n",
    "- ✅ `app/core/config.py`: Actualizado property `gemini_config`\n",
    "- ✅ Notebook: Actualizado código de conexión\n",
    "- ✅ Documentación: Añadida explicación técnica\n",
    "\n",
    "### 🚨 Impacto:\n",
    "- **Antes**: Posibles errores silenciosos con parámetro incorrecto\n",
    "- **Después**: Configuración precisa y compatible con Google Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0233e2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All project-specific dependencies imported successfully\n",
      "  - LangChain Google GenAI: ✓\n",
      "  - LangGraph: ✓\n",
      "  - PostgreSQL driver: ✓\n",
      "  - NumPy & Pandas: ✓\n"
     ]
    }
   ],
   "source": [
    "# Test specific project dependencies\n",
    "try:\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    from langgraph.graph import StateGraph\n",
    "    import psycopg2\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    print(\"✅ All project-specific dependencies imported successfully\")\n",
    "    print(\"  - LangChain Google GenAI: ✓\")\n",
    "    print(\"  - LangGraph: ✓\") \n",
    "    print(\"  - PostgreSQL driver: ✓\")\n",
    "    print(\"  - NumPy & Pandas: ✓\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Project dependency error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3e13c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔄 VERIFICACIÓN POST-REINICIO: Configuración con gemini-pro\n",
    "print(\"=\" * 60)\n",
    "print(\"🔄 VERIFICACIÓN DE MIGRACIÓN A GEMINI-PRO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Importar configuración fresh (después del reinicio de Docker)\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Reload modules to get fresh configuration\n",
    "if 'app.core.config' in sys.modules:\n",
    "    importlib.reload(sys.modules['app.core.config'])\n",
    "\n",
    "from app.core.config import get_settings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "try:\n",
    "    # Obtener configuración actualizada\n",
    "    settings = get_settings()\n",
    "    gemini_config = settings.gemini_config\n",
    "    \n",
    "    print(f\"🔧 Configuración Actual:\")\n",
    "    print(f\"   📱 Modelo: {gemini_config['model_name']} {'✅' if gemini_config['model_name'] == 'gemini-pro' else '❌'}\")\n",
    "    print(f\"   🌡️  Temperatura: {gemini_config['temperature']}\")\n",
    "    print(f\"   📊 Max output tokens: {gemini_config['max_output_tokens']}\")\n",
    "    print(f\"   🔑 API Key: {'✅ Configurada' if gemini_config['api_key'] else '❌ Faltante'}\")\n",
    "    print()\n",
    "    \n",
    "    if gemini_config['model_name'] == 'gemini-pro':\n",
    "        print(\"✅ MIGRACIÓN EXITOSA: Modelo actualizado a gemini-pro\")\n",
    "        \n",
    "        # Test connection with new model using correct parameter\n",
    "        print(\"🔍 Probando conexión con gemini-pro...\")\n",
    "        llm = ChatGoogleGenerativeAI(\n",
    "            model=gemini_config[\"model_name\"],\n",
    "            temperature=gemini_config[\"temperature\"], \n",
    "            max_output_tokens=gemini_config[\"max_output_tokens\"],  # Parámetro correcto para Google Gemini\n",
    "            google_api_key=gemini_config[\"api_key\"]\n",
    "        )\n",
    "        \n",
    "        response = llm.invoke(\"Hello! Test connection with gemini-pro model.\")\n",
    "        print(\"✅ Conexión exitosa con gemini-pro!\")\n",
    "        print(f\"📝 Respuesta: {response.content[:150]}...\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"❌ MIGRACIÓN FALLIDA: Aún usando {gemini_config['model_name']}\")\n",
    "        print(\"💡 Puede necesitar reiniciar el kernel del notebook\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error en la verificación: {e}\")\n",
    "    if \"429\" in str(e):\n",
    "        print(\"💡 Error de cuota - esperado si gemini-1.5-pro sigue activo\")\n",
    "        print(\"🔄 Reinicia el kernel del notebook para aplicar cambios\")\n",
    "    else:\n",
    "        print(\"💡 Verifica la configuración del API key\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
