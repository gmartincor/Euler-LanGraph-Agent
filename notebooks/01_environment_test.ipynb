{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a908d626",
   "metadata": {},
   "source": [
    "# Environment Test\n",
    "Verify all dependencies and API connections are working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56507e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.13 (main, Jul 23 2025, 18:09:53) [GCC 12.2.0]\n",
      "Google API Key configured: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Google API Key configured: {'GOOGLE_API_KEY' in os.environ}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32163f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All core dependencies imported successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import langchain\n",
    "    import langgraph\n",
    "    import streamlit\n",
    "    import matplotlib\n",
    "    import plotly\n",
    "    print(\"‚úÖ All core dependencies imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08a925e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Configuraci√≥n detectada:\n",
      "   Modelo: gemini-pro\n",
      "   Temperatura: 0.1\n",
      "   Max output tokens: 2048\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised NotFound: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Google Gemini API error: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "üí° Verifica que GOOGLE_API_KEY est√© configurada correctamente\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ CONFIGURACI√ìN ACTUALIZADA: Usando gemini-pro para optimizar rate limits\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from app.core.config import get_settings\n",
    "\n",
    "try:\n",
    "    # Usar la configuraci√≥n centralizada como debe ser\n",
    "    settings = get_settings()\n",
    "    gemini_config = settings.gemini_config\n",
    "    \n",
    "    print(f\"üîß Configuraci√≥n detectada:\")\n",
    "    print(f\"   Modelo: {gemini_config['model_name']}\")\n",
    "    print(f\"   Temperatura: {gemini_config['temperature']}\")\n",
    "    print(f\"   Max output tokens: {gemini_config['max_output_tokens']}\")\n",
    "    print()\n",
    "    \n",
    "    # Crear LLM con la configuraci√≥n correcta para Google Gemini\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=gemini_config[\"model_name\"],  # gemini-pro (actualizado para mejor rate limiting)\n",
    "        temperature=gemini_config[\"temperature\"],\n",
    "        max_output_tokens=gemini_config[\"max_output_tokens\"],  # Par√°metro correcto para Google Gemini\n",
    "        google_api_key=gemini_config[\"api_key\"]\n",
    "    )\n",
    "    \n",
    "    response = llm.invoke(\"Hello, test connection\")\n",
    "    print(\"‚úÖ Google Gemini API connection successful\")\n",
    "    print(f\"Response: {response.content[:100]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Google Gemini API error: {e}\")\n",
    "    print(\"üí° Verifica que GOOGLE_API_KEY est√© configurada correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4534b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üèÅ ENVIRONMENT TEST SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"üêç Python Version: {sys.version.split()[0]}\")\n",
    "print(f\"üê≥ Docker Environment: {'‚úÖ YES' if os.getenv('DATABASE_URL') else '‚ùå NO'}\")\n",
    "print(f\"üîë Google API Key: {'‚úÖ CONFIGURED' if os.getenv('GOOGLE_API_KEY') else '‚ùå MISSING'}\")\n",
    "print(f\"üóÑÔ∏è  Database URL: {'‚úÖ CONFIGURED' if os.getenv('DATABASE_URL') else '‚ùå MISSING'}\")\n",
    "print(\"\\nüí° Si todo est√° ‚úÖ, tu entorno est√° listo para el ReAct Agent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634fa328",
   "metadata": {},
   "source": [
    "## üéØ Actualizaci√≥n del Modelo Gemini\n",
    "\n",
    "**‚úÖ MIGRACI√ìN COMPLETADA**: Cambiado de `gemini-1.5-pro` a `gemini-pro` por optimizaci√≥n de rate limits\n",
    "\n",
    "### üìä Motivos del Cambio:\n",
    "- **Rate Limits Mejorados**: `gemini-pro` ofrece 60 requests/minuto vs l√≠mites m√°s estrictos de `1.5-pro`\n",
    "- **Costos Optimizados**: Modelo m√°s econ√≥mico para casos de uso de desarrollo y testing\n",
    "- **Compatibilidad**: Totalmente compatible con el stack actual de LangChain\n",
    "\n",
    "### üîß Cambios Realizados:\n",
    "- ‚úÖ Configuraci√≥n principal (`.env`) actualizada\n",
    "- ‚úÖ Configuraci√≥n por defecto (`config.py`) modificada  \n",
    "- ‚úÖ Tests unitarios actualizados\n",
    "- ‚úÖ Archivos de ejemplo sincronizados\n",
    "- ‚úÖ Documentaci√≥n actualizada\n",
    "\n",
    "### üöÄ Pr√≥ximos Pasos:\n",
    "1. **Reiniciar servicios**: `make restart` para aplicar cambios\n",
    "2. **Ejecutar tests**: `make test` para validar funcionalidad\n",
    "3. **Monitorear performance**: Verificar que el nuevo modelo cumple expectativas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cb9c23",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Actualizaci√≥n T√©cnica: Migraci√≥n a Gemini 1.5 Flash\n",
    "\n",
    "**üîß ACTUALIZACI√ìN APLICADA**: Migraci√≥n de `gemini-pro` a `gemini-1.5-flash`\n",
    "\n",
    "### üìã Mejoras Implementadas:\n",
    "- **Modelo**: `gemini-pro` ‚Üí `gemini-1.5-flash` (m√°s r√°pido y eficiente)\n",
    "- **Max Tokens**: Optimizado para 8192 tokens de salida\n",
    "- **Dependencias**: LangChain 0.3.72 para mejor compatibilidad\n",
    "- **Safety Settings**: Configuraci√≥n de seguridad mejorada\n",
    "\n",
    "### ‚úÖ Cambios Aplicados:\n",
    "- ‚úÖ `pyproject.toml`: Actualizadas dependencias LangChain\n",
    "- ‚úÖ `app/core/config.py`: Migraci√≥n a Gemini 1.5 Flash\n",
    "- ‚úÖ `app/agents/chains.py`: Configuraci√≥n optimizada del LLM\n",
    "- ‚úÖ `.env.example`: Valores por defecto actualizados\n",
    "- ‚úÖ Tests: Actualizados mocks para 1.5 Flash\n",
    "\n",
    "### \ude80 Beneficios:\n",
    "- **Rendimiento**: Gemini 1.5 Flash es m√°s r√°pido\n",
    "- **Compatibilidad**: Dependencias optimizadas\n",
    "- **Estabilidad**: Mejor manejo de errores y safety settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0233e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test specific project dependencies\n",
    "try:\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    from langgraph.graph import StateGraph\n",
    "    import psycopg2\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    print(\"‚úÖ All project-specific dependencies imported successfully\")\n",
    "    print(\"  - LangChain Google GenAI: ‚úì\")\n",
    "    print(\"  - LangGraph: ‚úì\") \n",
    "    print(\"  - PostgreSQL: ‚úì\")\n",
    "    print(\"  - NumPy: ‚úì\")\n",
    "    print(\"  - Pandas: ‚úì\")\n",
    "    \n",
    "    # Test Gemini 1.5 Flash configuration\n",
    "    gemini_config = {\n",
    "        \"model_name\": \"gemini-1.5-flash\",  # Updated to Gemini 1.5 Flash\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_output_tokens\": 8192,  # Flash's maximum tokens\n",
    "        \"api_key\": \"test-key\"\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nü§ñ Gemini Configuration:\")\n",
    "    print(f\"   Modelo: {gemini_config['model_name']}\")\n",
    "    print(f\"   Max Tokens: {gemini_config['max_output_tokens']}\")\n",
    "    print(f\"   Temperature: {gemini_config['temperature']}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Missing dependency: {e}\")\n",
    "    print(\"Run: pip install -r requirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3e13c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ CONFIGURACI√ìN ACTUALIZADA: Usando gemini-1.5-flash para mejor rendimiento\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "try:\n",
    "    # Get Google API key\n",
    "    google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if not google_api_key:\n",
    "        raise ValueError(\"GOOGLE_API_KEY not found in environment\")\n",
    "    \n",
    "    # Test Google Gemini 1.5 Flash connection\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\",  # Updated to Flash model\n",
    "        temperature=0.1,\n",
    "        max_output_tokens=8192,  # Flash's maximum tokens\n",
    "        api_key=google_api_key\n",
    "    )\n",
    "    \n",
    "    print(\"ü§ñ Testing Gemini 1.5 Flash connection...\")\n",
    "    response = llm.invoke(\"Hello! Test connection with gemini-1.5-flash model.\")\n",
    "    print(\"‚úÖ Connection successful!\")\n",
    "    print(f\"Response: {response.content[:100]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Google Gemini API error: {e}\")\n",
    "    print(\"   Check your GOOGLE_API_KEY in .env file\")\n",
    "    print(\"   Ensure your API key has access to Gemini 1.5 Flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4491bb39",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to restart the Kernel. \n",
      "\u001b[1;31mrequest to http://localhost:8888/api/kernels/df5fbd75-172b-4e7e-9b95-40039aec0c4a/restart?1754151882589 failed, reason: connect ECONNREFUSED ::1:8888. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# \udd04 VERIFICACI√ìN POST-MIGRACI√ìN: Configuraci√≥n con gemini-1.5-flash\n",
    "\n",
    "print(\"\udd04 VERIFICACI√ìN DE MIGRACI√ìN A GEMINI-1.5-FLASH\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    from app.core.config import get_settings\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    \n",
    "    # Load current settings\n",
    "    settings = get_settings()\n",
    "    gemini_config = settings.gemini_config\n",
    "    \n",
    "    print(\"\udccb Configuraci√≥n actual:\")\n",
    "    print(f\"   üì± Modelo: {gemini_config['model_name']} {'‚úÖ' if gemini_config['model_name'] == 'gemini-1.5-flash' else '‚ùå'}\")\n",
    "    print(f\"   üå°Ô∏è  Temperature: {gemini_config['temperature']}\")\n",
    "    print(f\"   üìä Max Tokens: {gemini_config['max_output_tokens']}\")\n",
    "    print(f\"   üîë API Key: {'‚úÖ Configurada' if gemini_config['api_key'] else '‚ùå Faltante'}\")\n",
    "    \n",
    "    # Verify migration success\n",
    "    if gemini_config['model_name'] == 'gemini-1.5-flash':\n",
    "        print(\"‚úÖ MIGRACI√ìN EXITOSA: Modelo actualizado a gemini-1.5-flash\")\n",
    "        print(\"üöÄ Beneficios: Mejor velocidad y eficiencia\")\n",
    "        \n",
    "        print(\"üîç Probando conexi√≥n con gemini-1.5-flash...\")\n",
    "        llm = ChatGoogleGenerativeAI(\n",
    "            model=gemini_config[\"model_name\"],\n",
    "            temperature=gemini_config[\"temperature\"],\n",
    "            max_output_tokens=gemini_config[\"max_output_tokens\"],\n",
    "            api_key=gemini_config[\"api_key\"]\n",
    "        )\n",
    "        \n",
    "        response = llm.invoke(\"Hello! Test connection with gemini-1.5-flash model.\")\n",
    "        print(\"‚úÖ Conexi√≥n exitosa con gemini-1.5-flash!\")\n",
    "        print(f\"\udcc4 Respuesta: {response.content[:50]}...\")\n",
    "    else:\n",
    "        print(\"‚ùå MIGRACI√ìN PENDIENTE: Actualizar configuraci√≥n a gemini-1.5-flash\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error durante verificaci√≥n: {e}\")\n",
    "    print(\"   Revisar configuraci√≥n en .env y app/core/config.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
