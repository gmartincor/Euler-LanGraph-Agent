{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f039577",
   "metadata": {},
   "source": [
    "# Error Detection and Debugging Notebook\n",
    "Systematic error detection, debugging, and resolution\n",
    "\n",
    "This notebook focuses on:\n",
    "- Error detection across all components\n",
    "- Debugging workflows and tools\n",
    "- Performance bottlenecks identification\n",
    "- Code quality analysis\n",
    "- Resolution strategies\n",
    "\n",
    "Following engineering principles: systematic debugging, root cause analysis, preventive measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14601681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment and comprehensive imports\n",
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "import traceback\n",
    "import inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Add app to path for imports\n",
    "sys.path.append(str(Path(\"..\").resolve()))\n",
    "\n",
    "# Core imports for comprehensive testing\n",
    "from app.core.config import get_settings\n",
    "from app.core.logging import get_logger, setup_logging\n",
    "from app.core.exceptions import AgentError, ToolError\n",
    "\n",
    "# Setup enhanced logging for debugging\n",
    "setup_logging()\n",
    "logger = get_logger(\"notebook_debug\")\n",
    "\n",
    "# Set more verbose logging for debugging\n",
    "logging.getLogger().setLevel(logging.DEBUG)\n",
    "\n",
    "logger.info(\"ğŸ› Starting comprehensive error detection and debugging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2674ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug 1: Configuration and Environment Validation\n",
    "def debug_configuration():\n",
    "    \"\"\"Comprehensive configuration and environment debugging.\"\"\"\n",
    "    \n",
    "    logger.info(\"ğŸ” Debugging configuration and environment...\")\n",
    "    \n",
    "    issues_found = []\n",
    "    recommendations = []\n",
    "    \n",
    "    try:\n",
    "        # Test settings loading\n",
    "        settings = get_settings()\n",
    "        logger.info(\"âœ… Settings loaded successfully\")\n",
    "        \n",
    "        # Validate critical settings\n",
    "        critical_settings = [\n",
    "            (\"GOOGLE_API_KEY\", \"Google API key for LLM\"),\n",
    "            (\"DATABASE_URL\", \"Database connection\"),\n",
    "            (\"ENVIRONMENT\", \"Environment configuration\")\n",
    "        ]\n",
    "        \n",
    "        for setting, description in critical_settings:\n",
    "            if hasattr(settings, setting.lower()) or setting in os.environ:\n",
    "                logger.info(f\"âœ… {description} configured\")\n",
    "            else:\n",
    "                issue = f\"âŒ Missing {description} ({setting})\"\n",
    "                issues_found.append(issue)\n",
    "                recommendations.append(f\"Set {setting} environment variable\")\n",
    "                logger.warning(issue)\n",
    "        \n",
    "        # Test LLM configuration\n",
    "        try:\n",
    "            gemini_config = settings.gemini_config\n",
    "            required_keys = [\"model_name\", \"temperature\", \"max_output_tokens\", \"api_key\"]\n",
    "            \n",
    "            for key in required_keys:\n",
    "                if key not in gemini_config or not gemini_config[key]:\n",
    "                    issue = f\"âŒ Missing Gemini config: {key}\"\n",
    "                    issues_found.append(issue)\n",
    "                    logger.warning(issue)\n",
    "                else:\n",
    "                    logger.info(f\"âœ… Gemini {key} configured\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            issue = f\"âŒ Gemini configuration error: {e}\"\n",
    "            issues_found.append(issue)\n",
    "            recommendations.append(\"Check Gemini LLM configuration in settings\")\n",
    "            logger.error(issue)\n",
    "        \n",
    "        # Test database configuration\n",
    "        try:\n",
    "            db_url = getattr(settings, 'database_url', os.getenv('DATABASE_URL'))\n",
    "            if db_url:\n",
    "                logger.info(\"âœ… Database URL configured\")\n",
    "                \n",
    "                # Test database connection\n",
    "                import psycopg2\n",
    "                try:\n",
    "                    # Parse database URL or use default connection\n",
    "                    if db_url.startswith('postgresql://'):\n",
    "                        conn = psycopg2.connect(db_url)\n",
    "                    else:\n",
    "                        # Use default Docker connection\n",
    "                        conn = psycopg2.connect(\n",
    "                            host=\"postgres\",\n",
    "                            database=\"react_agent_db\",\n",
    "                            user=\"agent_user\", \n",
    "                            password=\"agent_pass\",\n",
    "                            port=\"5432\"\n",
    "                        )\n",
    "                    conn.close()\n",
    "                    logger.info(\"âœ… Database connection successful\")\n",
    "                except Exception as db_e:\n",
    "                    issue = f\"âŒ Database connection failed: {db_e}\"\n",
    "                    issues_found.append(issue)\n",
    "                    recommendations.append(\"Ensure PostgreSQL container is running (make up)\")\n",
    "                    logger.error(issue)\n",
    "            else:\n",
    "                issue = \"âŒ Database URL not configured\"\n",
    "                issues_found.append(issue)\n",
    "                recommendations.append(\"Set DATABASE_URL or ensure Docker environment\")\n",
    "                logger.warning(issue)\n",
    "                \n",
    "        except Exception as e:\n",
    "            issue = f\"âŒ Database configuration error: {e}\"\n",
    "            issues_found.append(issue)\n",
    "            logger.error(issue)\n",
    "    \n",
    "    except Exception as e:\n",
    "        issue = f\"âŒ Critical configuration error: {e}\"\n",
    "        issues_found.append(issue)\n",
    "        recommendations.append(\"Check core configuration setup\")\n",
    "        logger.error(issue)\n",
    "        logger.error(traceback.format_exc())\n",
    "    \n",
    "    return {\n",
    "        \"issues_found\": issues_found,\n",
    "        \"recommendations\": recommendations,\n",
    "        \"status\": \"healthy\" if not issues_found else \"issues_detected\"\n",
    "    }\n",
    "\n",
    "# Run configuration debugging\n",
    "config_debug_result = debug_configuration()\n",
    "print(f\"Configuration status: {config_debug_result['status']}\")\n",
    "if config_debug_result['issues_found']:\n",
    "    print(\"Issues found:\")\n",
    "    for issue in config_debug_result['issues_found']:\n",
    "        print(f\"  {issue}\")\n",
    "    print(\"Recommendations:\")\n",
    "    for rec in config_debug_result['recommendations']:\n",
    "        print(f\"  - {rec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4ccdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug 2: Import and Dependency Validation\n",
    "def debug_imports_and_dependencies():\n",
    "    \"\"\"Debug import issues and dependency problems.\"\"\"\n",
    "    \n",
    "    logger.info(\"ğŸ” Debugging imports and dependencies...\")\n",
    "    \n",
    "    import_issues = []\n",
    "    dependency_status = {}\n",
    "    \n",
    "    # Critical dependencies to test\n",
    "    critical_imports = [\n",
    "        (\"langchain\", \"LangChain core\"),\n",
    "        (\"langgraph\", \"LangGraph framework\"),\n",
    "        (\"langchain_google_genai\", \"Google Gemini integration\"),\n",
    "        (\"streamlit\", \"Web interface\"),\n",
    "        (\"matplotlib\", \"Plotting\"),\n",
    "        (\"plotly\", \"Interactive plotting\"),\n",
    "        (\"psycopg2\", \"PostgreSQL driver\"),\n",
    "        (\"sympy\", \"Symbolic mathematics\"),\n",
    "        (\"numpy\", \"Numerical computing\"),\n",
    "        (\"pandas\", \"Data manipulation\")\n",
    "    ]\n",
    "    \n",
    "    for module_name, description in critical_imports:\n",
    "        try:\n",
    "            __import__(module_name)\n",
    "            dependency_status[module_name] = \"âœ… Available\"\n",
    "            logger.info(f\"âœ… {description} ({module_name}) imported successfully\")\n",
    "        except ImportError as e:\n",
    "            issue = f\"âŒ Failed to import {description} ({module_name}): {e}\"\n",
    "            import_issues.append(issue)\n",
    "            dependency_status[module_name] = f\"âŒ Failed: {str(e)}\"\n",
    "            logger.error(issue)\n",
    "        except Exception as e:\n",
    "            issue = f\"âŒ Unexpected error importing {description} ({module_name}): {e}\"\n",
    "            import_issues.append(issue)\n",
    "            dependency_status[module_name] = f\"âŒ Error: {str(e)}\"\n",
    "            logger.error(issue)\n",
    "    \n",
    "    # Test app module imports\n",
    "    app_modules = [\n",
    "        (\"app.core.config\", \"Core configuration\"),\n",
    "        (\"app.agents.graph\", \"Agent graph\"),\n",
    "        (\"app.agents.state\", \"Agent state\"),\n",
    "        (\"app.tools.registry\", \"Tool registry\"),\n",
    "        (\"app.tools.integral_tool\", \"Integral tool\"),\n",
    "        (\"app.tools.plot_tool\", \"Plot tool\"),\n",
    "        (\"app.tools.analysis_tool\", \"Analysis tool\"),\n",
    "        (\"app.models.agent_state\", \"Agent state model\"),\n",
    "        (\"app.ui.components.chat\", \"Chat component\")\n",
    "    ]\n",
    "    \n",
    "    for module_name, description in app_modules:\n",
    "        try:\n",
    "            __import__(module_name)\n",
    "            dependency_status[module_name] = \"âœ… Available\"\n",
    "            logger.info(f\"âœ… {description} ({module_name}) imported successfully\")\n",
    "        except ImportError as e:\n",
    "            issue = f\"âŒ Failed to import {description} ({module_name}): {e}\"\n",
    "            import_issues.append(issue)\n",
    "            dependency_status[module_name] = f\"âŒ Failed: {str(e)}\"\n",
    "            logger.error(issue)\n",
    "        except Exception as e:\n",
    "            issue = f\"âŒ Unexpected error importing {description} ({module_name}): {e}\"\n",
    "            import_issues.append(issue)\n",
    "            dependency_status[module_name] = f\"âŒ Error: {str(e)}\"\n",
    "            logger.error(issue)\n",
    "    \n",
    "    # Check Python version compatibility\n",
    "    python_version = sys.version_info\n",
    "    if python_version.major == 3 and python_version.minor >= 8:\n",
    "        logger.info(f\"âœ… Python version {python_version.major}.{python_version.minor} is compatible\")\n",
    "    else:\n",
    "        issue = f\"âŒ Python version {python_version.major}.{python_version.minor} may not be compatible (requires 3.8+)\"\n",
    "        import_issues.append(issue)\n",
    "        logger.warning(issue)\n",
    "    \n",
    "    return {\n",
    "        \"import_issues\": import_issues,\n",
    "        \"dependency_status\": dependency_status,\n",
    "        \"python_version\": f\"{python_version.major}.{python_version.minor}.{python_version.micro}\",\n",
    "        \"status\": \"healthy\" if not import_issues else \"issues_detected\"\n",
    "    }\n",
    "\n",
    "# Run import debugging\n",
    "import_debug_result = debug_imports_and_dependencies()\n",
    "print(f\"Import status: {import_debug_result['status']}\")\n",
    "print(f\"Python version: {import_debug_result['python_version']}\")\n",
    "\n",
    "if import_debug_result['import_issues']:\n",
    "    print(\"\\nImport issues found:\")\n",
    "    for issue in import_debug_result['import_issues']:\n",
    "        print(f\"  {issue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208185eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug 3: Component Integration Issues\n",
    "async def debug_component_integration():\n",
    "    \"\"\"Debug integration issues between components.\"\"\"\n",
    "    \n",
    "    logger.info(\"ğŸ” Debugging component integration...\")\n",
    "    \n",
    "    integration_issues = []\n",
    "    component_status = {}\n",
    "    \n",
    "    # Test 1: Tool Integration\n",
    "    try:\n",
    "        logger.info(\"Testing tool integration...\")\n",
    "        from app.tools.registry import ToolRegistry\n",
    "        from app.tools.integral_tool import IntegralTool\n",
    "        \n",
    "        # Create registry and tool\n",
    "        registry = ToolRegistry()\n",
    "        integral_tool = IntegralTool()\n",
    "        \n",
    "        # Test tool registration\n",
    "        registry.register_tool(\n",
    "            integral_tool,\n",
    "            categories=[\"mathematical\"],\n",
    "            tags=[\"integration\"]\n",
    "        )\n",
    "        \n",
    "        # Test tool retrieval\n",
    "        retrieved_tool = registry.get_tool(\"integral_calculator\")\n",
    "        if retrieved_tool:\n",
    "            component_status[\"tool_registry\"] = \"âœ… Working\"\n",
    "            logger.info(\"âœ… Tool registry integration working\")\n",
    "        else:\n",
    "            issue = \"âŒ Tool registry: tool not found after registration\"\n",
    "            integration_issues.append(issue)\n",
    "            component_status[\"tool_registry\"] = \"âŒ Failed\"\n",
    "            logger.error(issue)\n",
    "            \n",
    "    except Exception as e:\n",
    "        issue = f\"âŒ Tool integration error: {e}\"\n",
    "        integration_issues.append(issue)\n",
    "        component_status[\"tool_registry\"] = f\"âŒ Error: {str(e)}\"\n",
    "        logger.error(issue)\n",
    "        logger.error(traceback.format_exc())\n",
    "    \n",
    "    # Test 2: Agent State Integration\n",
    "    try:\n",
    "        logger.info(\"Testing agent state integration...\")\n",
    "        from app.agents.state import MathAgentState, WorkflowSteps, WorkflowStatus\n",
    "        from uuid import uuid4\n",
    "        \n",
    "        # Create test state\n",
    "        test_state = MathAgentState(\n",
    "            messages=[],\n",
    "            conversation_id=uuid4(),\n",
    "            session_id=\"debug_test\",\n",
    "            user_id=\"debug_user\",\n",
    "            created_at=datetime.now(),\n",
    "            updated_at=datetime.now(),\n",
    "            current_step=WorkflowSteps.PROBLEM_ANALYSIS,\n",
    "            iteration_count=0,\n",
    "            max_iterations=10,\n",
    "            workflow_status=WorkflowStatus.ACTIVE,\n",
    "            user_input=\"Test integration\",\n",
    "            problem_type=\"integration\",\n",
    "            reasoning_trace=[],\n",
    "            tool_calls=[],\n",
    "            final_result=None,\n",
    "            error_info=None,\n",
    "            memory=None,\n",
    "            visualization_data=None,\n",
    "            metadata={}\n",
    "        )\n",
    "        \n",
    "        # Validate state structure\n",
    "        if test_state[\"conversation_id\"] and test_state[\"session_id\"]:\n",
    "            component_status[\"agent_state\"] = \"âœ… Working\"\n",
    "            logger.info(\"âœ… Agent state integration working\")\n",
    "        else:\n",
    "            issue = \"âŒ Agent state: invalid state structure\"\n",
    "            integration_issues.append(issue)\n",
    "            component_status[\"agent_state\"] = \"âŒ Failed\"\n",
    "            logger.error(issue)\n",
    "            \n",
    "    except Exception as e:\n",
    "        issue = f\"âŒ Agent state integration error: {e}\"\n",
    "        integration_issues.append(issue)\n",
    "        component_status[\"agent_state\"] = f\"âŒ Error: {str(e)}\"\n",
    "        logger.error(issue)\n",
    "        logger.error(traceback.format_exc())\n",
    "    \n",
    "    # Test 3: LLM Integration\n",
    "    try:\n",
    "        logger.info(\"Testing LLM integration...\")\n",
    "        from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "        from app.core.config import get_settings\n",
    "        \n",
    "        settings = get_settings()\n",
    "        gemini_config = settings.gemini_config\n",
    "        \n",
    "        # Create LLM instance\n",
    "        llm = ChatGoogleGenerativeAI(\n",
    "            model=gemini_config[\"model_name\"],\n",
    "            temperature=gemini_config[\"temperature\"],\n",
    "            max_output_tokens=gemini_config[\"max_output_tokens\"],\n",
    "            google_api_key=gemini_config[\"api_key\"]\n",
    "        )\n",
    "        \n",
    "        # Test simple invocation\n",
    "        response = await asyncio.wait_for(\n",
    "            llm.ainvoke(\"Test connection. Respond with 'OK' only.\"),\n",
    "            timeout=10.0\n",
    "        )\n",
    "        \n",
    "        if response and hasattr(response, 'content'):\n",
    "            component_status[\"llm_integration\"] = \"âœ… Working\"\n",
    "            logger.info(f\"âœ… LLM integration working: {response.content[:50]}...\")\n",
    "        else:\n",
    "            issue = \"âŒ LLM integration: no valid response\"\n",
    "            integration_issues.append(issue)\n",
    "            component_status[\"llm_integration\"] = \"âŒ Failed\"\n",
    "            logger.error(issue)\n",
    "            \n",
    "    except asyncio.TimeoutError:\n",
    "        issue = \"âŒ LLM integration: timeout (network/API issue)\"\n",
    "        integration_issues.append(issue)\n",
    "        component_status[\"llm_integration\"] = \"âŒ Timeout\"\n",
    "        logger.error(issue)\n",
    "    except Exception as e:\n",
    "        issue = f\"âŒ LLM integration error: {e}\"\n",
    "        integration_issues.append(issue)\n",
    "        component_status[\"llm_integration\"] = f\"âŒ Error: {str(e)}\"\n",
    "        logger.error(issue)\n",
    "        logger.error(traceback.format_exc())\n",
    "    \n",
    "    # Test 4: Database Integration\n",
    "    try:\n",
    "        logger.info(\"Testing database integration...\")\n",
    "        from app.database.connection import get_connection\n",
    "        \n",
    "        # Test database connection\n",
    "        conn = await get_connection()\n",
    "        if conn:\n",
    "            await conn.close()\n",
    "            component_status[\"database\"] = \"âœ… Working\"\n",
    "            logger.info(\"âœ… Database integration working\")\n",
    "        else:\n",
    "            issue = \"âŒ Database integration: connection failed\"\n",
    "            integration_issues.append(issue)\n",
    "            component_status[\"database\"] = \"âŒ Failed\"\n",
    "            logger.error(issue)\n",
    "            \n",
    "    except Exception as e:\n",
    "        issue = f\"âŒ Database integration error: {e}\"\n",
    "        integration_issues.append(issue)\n",
    "        component_status[\"database\"] = f\"âŒ Error: {str(e)}\"\n",
    "        logger.error(issue)\n",
    "    \n",
    "    return {\n",
    "        \"integration_issues\": integration_issues,\n",
    "        \"component_status\": component_status,\n",
    "        \"status\": \"healthy\" if not integration_issues else \"issues_detected\"\n",
    "    }\n",
    "\n",
    "# Run component integration debugging\n",
    "integration_debug_result = await debug_component_integration()\n",
    "print(f\"Integration status: {integration_debug_result['status']}\")\n",
    "print(\"\\nComponent status:\")\n",
    "for component, status in integration_debug_result['component_status'].items():\n",
    "    print(f\"  {component}: {status}\")\n",
    "\n",
    "if integration_debug_result['integration_issues']:\n",
    "    print(\"\\nIntegration issues:\")\n",
    "    for issue in integration_debug_result['integration_issues']:\n",
    "        print(f\"  {issue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3ba1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug 4: Workflow Execution Issues\n",
    "async def debug_workflow_execution():\n",
    "    \"\"\"Debug workflow execution and identify bottlenecks.\"\"\"\n",
    "    \n",
    "    logger.info(\"ğŸ” Debugging workflow execution...\")\n",
    "    \n",
    "    workflow_issues = []\n",
    "    execution_metrics = {}\n",
    "    \n",
    "    try:\n",
    "        # Test workflow creation\n",
    "        logger.info(\"Testing workflow creation...\")\n",
    "        from app.agents.graph import create_mathematical_workflow\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        workflow = await create_mathematical_workflow()\n",
    "        creation_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        execution_metrics[\"workflow_creation_time\"] = creation_time\n",
    "        logger.info(f\"âœ… Workflow created in {creation_time:.2f}s\")\n",
    "        \n",
    "        # Test simple workflow execution\n",
    "        logger.info(\"Testing simple workflow execution...\")\n",
    "        from app.agents.state import MathAgentState, WorkflowSteps, WorkflowStatus\n",
    "        from uuid import uuid4\n",
    "        \n",
    "        test_state = MathAgentState(\n",
    "            messages=[],\n",
    "            conversation_id=uuid4(),\n",
    "            session_id=\"debug_workflow\",\n",
    "            user_id=\"debug_user\",\n",
    "            created_at=datetime.now(),\n",
    "            updated_at=datetime.now(),\n",
    "            current_step=WorkflowSteps.PROBLEM_ANALYSIS,\n",
    "            iteration_count=0,\n",
    "            max_iterations=5,  # Reduced for debugging\n",
    "            workflow_status=WorkflowStatus.ACTIVE,\n",
    "            user_input=\"Calculate integral of x^2\",\n",
    "            problem_type=None,\n",
    "            reasoning_trace=[],\n",
    "            tool_calls=[],\n",
    "            final_result=None,\n",
    "            error_info=None,\n",
    "            memory=None,\n",
    "            visualization_data=None,\n",
    "            metadata={\"debug\": True}\n",
    "        )\n",
    "        \n",
    "        # Execute workflow with timeout and monitoring\n",
    "        execution_start = datetime.now()\n",
    "        step_count = 0\n",
    "        last_state = None\n",
    "        execution_trace = []\n",
    "        \n",
    "        try:\n",
    "            async for state in workflow.astream(test_state):\n",
    "                step_count += 1\n",
    "                current_step = state.get(\"current_step\", \"unknown\")\n",
    "                step_time = datetime.now()\n",
    "                \n",
    "                execution_trace.append({\n",
    "                    \"step\": step_count,\n",
    "                    \"node\": current_step,\n",
    "                    \"timestamp\": step_time.isoformat(),\n",
    "                    \"status\": state.get(\"workflow_status\", \"unknown\")\n",
    "                })\n",
    "                \n",
    "                logger.info(f\"ğŸ“ Debug Step {step_count}: {current_step}\")\n",
    "                \n",
    "                # Check for infinite loops or excessive iterations\n",
    "                if step_count > 15:\n",
    "                    issue = f\"âŒ Workflow execution: excessive iterations ({step_count})\"\n",
    "                    workflow_issues.append(issue)\n",
    "                    logger.warning(issue)\n",
    "                    break\n",
    "                \n",
    "                # Check for stuck states\n",
    "                if last_state and last_state.get(\"current_step\") == current_step:\n",
    "                    issue = f\"âŒ Workflow execution: stuck in state {current_step}\"\n",
    "                    workflow_issues.append(issue)\n",
    "                    logger.warning(issue)\n",
    "                \n",
    "                last_state = state\n",
    "            \n",
    "            execution_time = (datetime.now() - execution_start).total_seconds()\n",
    "            execution_metrics[\"workflow_execution_time\"] = execution_time\n",
    "            execution_metrics[\"steps_executed\"] = step_count\n",
    "            execution_metrics[\"execution_trace\"] = execution_trace\n",
    "            \n",
    "            # Validate final state\n",
    "            if last_state:\n",
    "                final_status = last_state.get(\"workflow_status\", \"unknown\")\n",
    "                has_result = last_state.get(\"final_result\") is not None\n",
    "                has_errors = last_state.get(\"error_info\") is not None\n",
    "                \n",
    "                execution_metrics[\"final_status\"] = final_status\n",
    "                execution_metrics[\"has_result\"] = has_result\n",
    "                execution_metrics[\"has_errors\"] = has_errors\n",
    "                \n",
    "                if final_status in [WorkflowStatus.COMPLETED, \"completed\"]:\n",
    "                    logger.info(f\"âœ… Workflow completed successfully in {execution_time:.2f}s\")\n",
    "                elif has_errors:\n",
    "                    issue = f\"âŒ Workflow completed with errors: {last_state.get('error_info')}\"\n",
    "                    workflow_issues.append(issue)\n",
    "                    logger.warning(issue)\n",
    "                else:\n",
    "                    issue = f\"âš ï¸ Workflow ended with status: {final_status}\"\n",
    "                    workflow_issues.append(issue)\n",
    "                    logger.warning(issue)\n",
    "            else:\n",
    "                issue = \"âŒ Workflow execution: no final state\"\n",
    "                workflow_issues.append(issue)\n",
    "                logger.error(issue)\n",
    "        \n",
    "        except asyncio.TimeoutError:\n",
    "            issue = \"âŒ Workflow execution: timeout\"\n",
    "            workflow_issues.append(issue)\n",
    "            logger.error(issue)\n",
    "        except Exception as e:\n",
    "            issue = f\"âŒ Workflow execution error: {e}\"\n",
    "            workflow_issues.append(issue)\n",
    "            logger.error(issue)\n",
    "            logger.error(traceback.format_exc())\n",
    "    \n",
    "    except Exception as e:\n",
    "        issue = f\"âŒ Workflow creation error: {e}\"\n",
    "        workflow_issues.append(issue)\n",
    "        logger.error(issue)\n",
    "        logger.error(traceback.format_exc())\n",
    "    \n",
    "    return {\n",
    "        \"workflow_issues\": workflow_issues,\n",
    "        \"execution_metrics\": execution_metrics,\n",
    "        \"status\": \"healthy\" if not workflow_issues else \"issues_detected\"\n",
    "    }\n",
    "\n",
    "# Run workflow debugging\n",
    "workflow_debug_result = await debug_workflow_execution()\n",
    "print(f\"Workflow status: {workflow_debug_result['status']}\")\n",
    "print(f\"Execution metrics: {workflow_debug_result['execution_metrics']}\")\n",
    "\n",
    "if workflow_debug_result['workflow_issues']:\n",
    "    print(\"\\nWorkflow issues:\")\n",
    "    for issue in workflow_debug_result['workflow_issues']:\n",
    "        print(f\"  {issue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dd91d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug 5: Performance and Resource Analysis\n",
    "import psutil\n",
    "import gc\n",
    "import time\n",
    "\n",
    "def debug_performance_and_resources():\n",
    "    \"\"\"Analyze performance and resource usage patterns.\"\"\"\n",
    "    \n",
    "    logger.info(\"ğŸ“ˆ Debugging performance and resources...\")\n",
    "    \n",
    "    performance_issues = []\n",
    "    resource_metrics = {}\n",
    "    \n",
    "    # Get system information\n",
    "    process = psutil.Process()\n",
    "    \n",
    "    # Memory analysis\n",
    "    memory_info = process.memory_info()\n",
    "    memory_percent = process.memory_percent()\n",
    "    \n",
    "    resource_metrics[\"memory_usage_mb\"] = memory_info.rss / 1024 / 1024\n",
    "    resource_metrics[\"memory_percent\"] = memory_percent\n",
    "    \n",
    "    # CPU analysis\n",
    "    cpu_percent = process.cpu_percent(interval=1)\n",
    "    resource_metrics[\"cpu_percent\"] = cpu_percent\n",
    "    \n",
    "    # Disk usage\n",
    "    disk_usage = psutil.disk_usage('/')\n",
    "    resource_metrics[\"disk_usage_percent\"] = (disk_usage.used / disk_usage.total) * 100\n",
    "    \n",
    "    logger.info(f\"Memory usage: {resource_metrics['memory_usage_mb']:.1f}MB ({memory_percent:.1f}%)\")\n",
    "    logger.info(f\"CPU usage: {cpu_percent:.1f}%\")\n",
    "    logger.info(f\"Disk usage: {resource_metrics['disk_usage_percent']:.1f}%\")\n",
    "    \n",
    "    # Check for performance issues\n",
    "    if memory_percent > 80:\n",
    "        issue = f\"âŒ High memory usage: {memory_percent:.1f}%\"\n",
    "        performance_issues.append(issue)\n",
    "        logger.warning(issue)\n",
    "    \n",
    "    if cpu_percent > 90:\n",
    "        issue = f\"âŒ High CPU usage: {cpu_percent:.1f}%\"\n",
    "        performance_issues.append(issue)\n",
    "        logger.warning(issue)\n",
    "    \n",
    "    if resource_metrics[\"disk_usage_percent\"] > 90:\n",
    "        issue = f\"âŒ High disk usage: {resource_metrics['disk_usage_percent']:.1f}%\"\n",
    "        performance_issues.append(issue)\n",
    "        logger.warning(issue)\n",
    "    \n",
    "    # Memory leak detection\n",
    "    gc.collect()  # Force garbage collection\n",
    "    \n",
    "    # Check for large objects in memory\n",
    "    import sys\n",
    "    large_objects = []\n",
    "    for obj in gc.get_objects():\n",
    "        size = sys.getsizeof(obj)\n",
    "        if size > 1024 * 1024:  # Objects larger than 1MB\n",
    "            large_objects.append({\n",
    "                \"type\": type(obj).__name__,\n",
    "                \"size_mb\": size / 1024 / 1024\n",
    "            })\n",
    "    \n",
    "    if large_objects:\n",
    "        resource_metrics[\"large_objects\"] = large_objects[:5]  # Top 5\n",
    "        logger.info(f\"Found {len(large_objects)} large objects in memory\")\n",
    "        for obj in large_objects[:3]:\n",
    "            logger.info(f\"  {obj['type']}: {obj['size_mb']:.1f}MB\")\n",
    "    \n",
    "    # Thread analysis\n",
    "    thread_count = process.num_threads()\n",
    "    resource_metrics[\"thread_count\"] = thread_count\n",
    "    \n",
    "    if thread_count > 50:\n",
    "        issue = f\"âŒ High thread count: {thread_count}\"\n",
    "        performance_issues.append(issue)\n",
    "        logger.warning(issue)\n",
    "    \n",
    "    # File descriptor analysis (Unix systems)\n",
    "    try:\n",
    "        fd_count = process.num_fds()\n",
    "        resource_metrics[\"file_descriptors\"] = fd_count\n",
    "        \n",
    "        if fd_count > 1000:\n",
    "            issue = f\"âŒ High file descriptor count: {fd_count}\"\n",
    "            performance_issues.append(issue)\n",
    "            logger.warning(issue)\n",
    "    except AttributeError:\n",
    "        # Windows doesn't have num_fds\n",
    "        pass\n",
    "    \n",
    "    return {\n",
    "        \"performance_issues\": performance_issues,\n",
    "        \"resource_metrics\": resource_metrics,\n",
    "        \"status\": \"healthy\" if not performance_issues else \"issues_detected\"\n",
    "    }\n",
    "\n",
    "# Run performance debugging\n",
    "performance_debug_result = debug_performance_and_resources()\n",
    "print(f\"Performance status: {performance_debug_result['status']}\")\n",
    "print(f\"Memory usage: {performance_debug_result['resource_metrics']['memory_usage_mb']:.1f}MB\")\n",
    "print(f\"CPU usage: {performance_debug_result['resource_metrics']['cpu_percent']:.1f}%\")\n",
    "\n",
    "if performance_debug_result['performance_issues']:\n",
    "    print(\"\\nPerformance issues:\")\n",
    "    for issue in performance_debug_result['performance_issues']:\n",
    "        print(f\"  {issue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0200f0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug Summary and Resolution Plan\n",
    "def generate_debug_summary():\n",
    "    \"\"\"Generate comprehensive debug summary and resolution plan.\"\"\"\n",
    "    \n",
    "    logger.info(\"ğŸ“‹ Generating debug summary and resolution plan...\")\n",
    "    \n",
    "    # Collect all debug results\n",
    "    all_results = {\n",
    "        \"configuration\": config_debug_result,\n",
    "        \"imports\": import_debug_result,\n",
    "        \"integration\": integration_debug_result,\n",
    "        \"workflow\": workflow_debug_result,\n",
    "        \"performance\": performance_debug_result\n",
    "    }\n",
    "    \n",
    "    # Categorize issues by severity\n",
    "    critical_issues = []\n",
    "    warning_issues = []\n",
    "    performance_issues = []\n",
    "    \n",
    "    for category, result in all_results.items():\n",
    "        if result.get(\"status\") == \"issues_detected\":\n",
    "            for issue_key in [\"issues_found\", \"import_issues\", \"integration_issues\", \"workflow_issues\", \"performance_issues\"]:\n",
    "                issues = result.get(issue_key, [])\n",
    "                for issue in issues:\n",
    "                    if \"âŒ\" in issue:\n",
    "                        if \"timeout\" in issue.lower() or \"connection\" in issue.lower():\n",
    "                            critical_issues.append(f\"[{category.upper()}] {issue}\")\n",
    "                        elif \"performance\" in issue.lower() or \"memory\" in issue.lower():\n",
    "                            performance_issues.append(f\"[{category.upper()}] {issue}\")\n",
    "                        else:\n",
    "                            critical_issues.append(f\"[{category.upper()}] {issue}\")\n",
    "                    elif \"âš ï¸\" in issue:\n",
    "                        warning_issues.append(f\"[{category.upper()}] {issue}\")\n",
    "    \n",
    "    # Generate resolution plan\n",
    "    resolution_plan = []\n",
    "    \n",
    "    # Critical issues first\n",
    "    if critical_issues:\n",
    "        resolution_plan.append(\"ğŸš¨ CRITICAL ISSUES (Fix Immediately):\")\n",
    "        for issue in critical_issues:\n",
    "            resolution_plan.append(f\"  {issue}\")\n",
    "            \n",
    "            # Add specific recommendations\n",
    "            if \"database\" in issue.lower():\n",
    "                resolution_plan.append(\"    â†’ Run 'make up' to start PostgreSQL container\")\n",
    "            elif \"api\" in issue.lower() or \"google\" in issue.lower():\n",
    "                resolution_plan.append(\"    â†’ Check GOOGLE_API_KEY environment variable\")\n",
    "            elif \"import\" in issue.lower():\n",
    "                resolution_plan.append(\"    â†’ Install missing dependencies with 'poetry install'\")\n",
    "            elif \"timeout\" in issue.lower():\n",
    "                resolution_plan.append(\"    â†’ Check network connectivity and API quotas\")\n",
    "    \n",
    "    # Warning issues\n",
    "    if warning_issues:\n",
    "        resolution_plan.append(\"\\nâš ï¸  WARNING ISSUES (Monitor and Fix):\")\n",
    "        for issue in warning_issues:\n",
    "            resolution_plan.append(f\"  {issue}\")\n",
    "    \n",
    "    # Performance issues\n",
    "    if performance_issues:\n",
    "        resolution_plan.append(\"\\nğŸ“ˆ PERFORMANCE ISSUES (Optimize):\")\n",
    "        for issue in performance_issues:\n",
    "            resolution_plan.append(f\"  {issue}\")\n",
    "    \n",
    "    # Overall system health\n",
    "    healthy_components = sum(1 for result in all_results.values() if result.get(\"status\") == \"healthy\")\n",
    "    total_components = len(all_results)\n",
    "    health_percentage = (healthy_components / total_components) * 100\n",
    "    \n",
    "    # Recommendations based on health\n",
    "    if health_percentage >= 80:\n",
    "        system_status = \"ğŸŸ¢ HEALTHY\"\n",
    "        resolution_plan.append(f\"\\n{system_status} - System is functioning well ({health_percentage:.0f}% components healthy)\")\n",
    "    elif health_percentage >= 60:\n",
    "        system_status = \"ğŸŸ¡ DEGRADED\"\n",
    "        resolution_plan.append(f\"\\n{system_status} - Some issues detected ({health_percentage:.0f}% components healthy)\")\n",
    "        resolution_plan.append(\"  â†’ Focus on critical issues first\")\n",
    "    else:\n",
    "        system_status = \"ğŸ”´ CRITICAL\"\n",
    "        resolution_plan.append(f\"\\n{system_status} - Major issues detected ({health_percentage:.0f}% components healthy)\")\n",
    "        resolution_plan.append(\"  â†’ Requires immediate attention\")\n",
    "    \n",
    "    # Next steps\n",
    "    resolution_plan.append(\"\\nğŸ“ RECOMMENDED NEXT STEPS:\")\n",
    "    resolution_plan.append(\"1. Fix critical issues in order of appearance\")\n",
    "    resolution_plan.append(\"2. Verify fixes by running relevant test notebooks\")\n",
    "    resolution_plan.append(\"3. Monitor performance metrics\")\n",
    "    resolution_plan.append(\"4. Run full integration tests\")\n",
    "    resolution_plan.append(\"5. Update documentation with findings\")\n",
    "    \n",
    "    summary = {\n",
    "        \"system_status\": system_status,\n",
    "        \"health_percentage\": health_percentage,\n",
    "        \"critical_issues_count\": len(critical_issues),\n",
    "        \"warning_issues_count\": len(warning_issues),\n",
    "        \"performance_issues_count\": len(performance_issues),\n",
    "        \"resolution_plan\": resolution_plan,\n",
    "        \"detailed_results\": all_results\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"ğŸ¯ Debug summary generated - System status: {system_status}\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate final summary\n",
    "debug_summary = generate_debug_summary()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ› COMPREHENSIVE DEBUG SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"System Status: {debug_summary['system_status']}\")\n",
    "print(f\"Health: {debug_summary['health_percentage']:.0f}%\")\n",
    "print(f\"Critical Issues: {debug_summary['critical_issues_count']}\")\n",
    "print(f\"Warnings: {debug_summary['warning_issues_count']}\")\n",
    "print(f\"Performance Issues: {debug_summary['performance_issues_count']}\")\n",
    "print(\"\\n\" + \"\\n\".join(debug_summary['resolution_plan']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e526f4b9",
   "metadata": {},
   "source": [
    "## Error Detection and Debugging Results\n",
    "\n",
    "This notebook performed comprehensive error detection and debugging across all system components:\n",
    "\n",
    "### System Health Overview\n",
    "- **Configuration**: Environment and settings validation\n",
    "- **Dependencies**: Import and module availability  \n",
    "- **Integration**: Component interaction testing\n",
    "- **Workflow**: End-to-end execution validation\n",
    "- **Performance**: Resource usage and optimization\n",
    "\n",
    "### Debug Categories Tested:\n",
    "1. âœ… **Configuration Validation**: Environment variables, API keys, settings\n",
    "2. âœ… **Import Analysis**: Dependency availability and compatibility\n",
    "3. âœ… **Integration Testing**: Component communication and data flow\n",
    "4. âœ… **Workflow Execution**: End-to-end process validation\n",
    "5. âœ… **Performance Analysis**: Resource usage and bottlenecks\n",
    "\n",
    "### Key Debugging Features:\n",
    "- ğŸ” **Systematic Error Detection**: Comprehensive component scanning\n",
    "- ğŸ› ï¸ **Root Cause Analysis**: Detailed error tracking with context\n",
    "- ğŸ“Š **Performance Monitoring**: Resource usage tracking\n",
    "- ğŸ¯ **Targeted Recommendations**: Specific fix suggestions\n",
    "- ğŸ“‹ **Resolution Planning**: Prioritized action items\n",
    "\n",
    "### Resolution Strategy:\n",
    "1. **Critical Issues First**: Database, API, core functionality\n",
    "2. **Warning Issues**: Non-blocking but important fixes\n",
    "3. **Performance Issues**: Optimization opportunities\n",
    "4. **Preventive Measures**: Monitoring and alerting setup\n",
    "\n",
    "### Debugging Tools Used:\n",
    "- Exception handling and tracebacks\n",
    "- Resource monitoring (CPU, memory, disk)\n",
    "- Component isolation testing\n",
    "- Integration validation\n",
    "- Performance profiling\n",
    "\n",
    "This systematic approach ensures all issues are identified, categorized, and resolved efficiently while maintaining code quality and system reliability."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
